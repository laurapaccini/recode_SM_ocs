{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tobac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable a few warnings:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, append=True)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, append=True)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, append=True)\n",
    "warnings.filterwarnings('ignore',category=pd.io.pytables.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lon(ds,longitude='lon'):\n",
    "    ds.coords[longitude] = (ds.coords[longitude] + 180) % 360 - 180\n",
    "    ds = ds.sortby(ds.lon)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imerg2015 = xr.open_dataset('../../IMERG/IMERG_hourly_2015.nc')\n",
    "imerg2016 = xr.open_dataset('../../IMERG/IMERG_hourly_2016.nc')\n",
    "imerg2017 = xr.open_dataset('../../IMERG/IMERG_hourly_2017.nc')\n",
    "imerg2018 = xr.open_dataset('../../IMERG/IMERG_hourly_2018.nc')\n",
    "imerg2019 = xr.open_dataset('../../IMERG/IMERG_hourly_2019.nc')\n",
    "imerg2020 = xr.open_dataset('../../IMERG/IMERG_hourly_2020.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(array,xarr,time=True,varname='mask'):\n",
    "    if time==True:\n",
    "        ds = xr.Dataset( { varname: ([\"time\",\"lat\", \"lon\"], array)},\n",
    "    coords={ \"time\":([\"time\"],xarr.time.values), \"lat\": ([\"lat\"], xarr.lat.values),\"lon\": ([\"lon\"], \n",
    "             xarr.lon.values)})\n",
    "    else:\n",
    "        ds = xr.Dataset( { varname: ([\"lat\", \"lon\"], array)},\n",
    "    coords={ \"lat\": ([\"lat\"], xarr.lat.values),\"lon\": ([\"lon\"], xarr.lon.values)}) \n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sel_by_lifetime(Track,hours=3,operator='>'):\n",
    "    \"\"\"\n",
    "    for hourly data\n",
    "    \"\"\"\n",
    "    counts = Track.groupby(\"cell\")[\"time_cell\"].count().values\n",
    "    if operator == '<':\n",
    "        counts_min = Track.groupby(\"cell\")[\"time_cell\"].count()[counts<hours]\n",
    "    elif operator == '>':\n",
    "        counts_min = Track.groupby(\"cell\")[\"time_cell\"].count()[counts>hours]\n",
    "    else:\n",
    "        raise ValueError('Invalid operator. Choose either \"<\" or \">\".')\n",
    "    selected_cells = Track[Track[\"cell\"].isin(counts_min.reset_index()[counts_min.reset_index().cell>0].cell)]\n",
    "    selected_cells = selected_cells.reset_index()\n",
    "    \n",
    "    return selected_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open objects (dataframes) \n",
    "\n",
    "## *************** Amazon region  ***************\n",
    "df_h2015 = pd.read_pickle('pkl_files/df_2015_imergmax.pkl')\n",
    "df_h2016 = pd.read_pickle('pkl_files/df_2016_imergmax.pkl') \n",
    "df_h2017 = pd.read_pickle('pkl_files/df_2017_imergmax.pkl') \n",
    "df_h2018 = pd.read_pickle('pkl_files/df_2018_imergmax.pkl') \n",
    "df_h2019 = pd.read_pickle('pkl_files/df_2019_imergmax.pkl') \n",
    "df_h2020 = pd.read_pickle('pkl_files/df_2020_imergmax.pkl') \n",
    "\n",
    "\n",
    "## *************** SESA region  ***************\n",
    "df_h2015_sesa = pd.read_pickle('pkl_files/df_2015_sesa_imergmax.pkl')\n",
    "df_h2016_sesa = pd.read_pickle('pkl_files/df_2016_sesa_imergmax.pkl') \n",
    "df_h2017_sesa = pd.read_pickle('pkl_files/df_2017_sesa_imergmax.pkl') \n",
    "df_h2018_sesa = pd.read_pickle('pkl_files/df_2018_sesa_imergmax.pkl') \n",
    "df_h2019_sesa = pd.read_pickle('pkl_files/df_2019_sesa_imergmax.pkl') \n",
    "df_h2020_sesa = pd.read_pickle('pkl_files/df_2020_sesa_imergmax.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename columns to prepare for TOBAC tracking\n",
    "\n",
    "## *************** Amazon region  ***************\n",
    "df_h2015 = df_h2015.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2015['feature'] += 1 # Add 1 to the values in the \"feature\" column\n",
    "df_h2016 = df_h2016.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2016['feature'] += 1\n",
    "df_h2017 = df_h2017.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2017['feature'] += 1\n",
    "df_h2018 = df_h2018.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2018['feature'] += 1\n",
    "df_h2019 = df_h2019.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2019['feature'] += 1\n",
    "df_h2020 = df_h2020.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2020['feature'] += 1\n",
    "\n",
    "df_h2015 = df_h2015.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2016 = df_h2016.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2017 = df_h2017.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2018 = df_h2018.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2019 = df_h2019.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2020 = df_h2020.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "\n",
    "\n",
    "## *************** SESA region  ***************\n",
    "df_h2015_sesa = df_h2015_sesa.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2015_sesa['feature'] += 1 # Add 1 to the values in the \"feature\" column\n",
    "df_h2016_sesa = df_h2016_sesa.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2016_sesa['feature'] += 1\n",
    "df_h2017_sesa = df_h2017_sesa.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2017_sesa['feature'] += 1\n",
    "df_h2018_sesa = df_h2018_sesa.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2018_sesa['feature'] += 1\n",
    "df_h2019_sesa = df_h2019_sesa.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2019_sesa['feature'] += 1\n",
    "df_h2020_sesa = df_h2020_sesa.reset_index(drop=True).reset_index().rename(columns={'index': 'feature'})\n",
    "df_h2020_sesa['feature'] += 1\n",
    "\n",
    "df_h2015_sesa = df_h2015_sesa.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2016_sesa = df_h2016_sesa.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2017_sesa = df_h2017_sesa.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2018_sesa = df_h2018_sesa.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2019_sesa = df_h2019_sesa.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n",
    "df_h2020_sesa = df_h2020_sesa.rename(columns={\"y\": \"hdim_1\", \"x\": \"hdim_2\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"frame\" column by grouping the data by unique values of \"time\"\n",
    "df_h2015['frame'] = df_h2015.groupby('time').ngroup()\n",
    "df_h2016['frame'] = df_h2016.groupby('time').ngroup()\n",
    "df_h2017['frame'] = df_h2017.groupby('time').ngroup()\n",
    "df_h2018['frame'] = df_h2018.groupby('time').ngroup()\n",
    "df_h2019['frame'] = df_h2019.groupby('time').ngroup()\n",
    "df_h2020['frame'] = df_h2020.groupby('time').ngroup()\n",
    "\n",
    "\n",
    "df_h2015_sesa['frame'] = df_h2015_sesa.groupby('time').ngroup()\n",
    "df_h2016_sesa['frame'] = df_h2016_sesa.groupby('time').ngroup()\n",
    "df_h2017_sesa['frame'] = df_h2017_sesa.groupby('time').ngroup()\n",
    "df_h2018_sesa['frame'] = df_h2018_sesa.groupby('time').ngroup()\n",
    "df_h2019_sesa['frame'] = df_h2019_sesa.groupby('time').ngroup()\n",
    "df_h2020_sesa['frame'] = df_h2020_sesa.groupby('time').ngroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid spacing of the input data (in meter) and time in seconds\n",
    "dxy = 9999 #(10km)\n",
    "dt = 3600 #200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary containing keyword arguments for the linking step:\n",
    "parameters_linking={}\n",
    "parameters_linking['method_linking']='predict'\n",
    "parameters_linking['adaptive_stop']=0.2\n",
    "parameters_linking['adaptive_step']=0.95\n",
    "parameters_linking['subnetwork_size']=100\n",
    "parameters_linking['memory']=0\n",
    "parameters_linking['time_cell_min']=5*60\n",
    "parameters_linking['v_max']=10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 2207: 26 trajectories present.\n"
     ]
    }
   ],
   "source": [
    "Track_h2015 = tobac.linking_trackpy(df_h2015,imerg2015.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2016 = tobac.linking_trackpy(df_h2016,imerg2016.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2017 = tobac.linking_trackpy(df_h2017,imerg2017.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2018 = tobac.linking_trackpy(df_h2018,imerg2018.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2019 = tobac.linking_trackpy(df_h2019,imerg2019.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2020 = tobac.linking_trackpy(df_h2020,imerg2020.precip,dt=dt,dxy=dxy,**parameters_linking);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1183: 1 trajectories present.\n"
     ]
    }
   ],
   "source": [
    "Track_h2015_sesa = tobac.linking_trackpy(df_h2015_sesa,imerg2015.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2016_sesa = tobac.linking_trackpy(df_h2016_sesa,imerg2016.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2017_sesa = tobac.linking_trackpy(df_h2017_sesa,imerg2017.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2018_sesa = tobac.linking_trackpy(df_h2018_sesa,imerg2018.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2019_sesa = tobac.linking_trackpy(df_h2019_sesa,imerg2019.precip,dt=dt,dxy=dxy,**parameters_linking);\n",
    "Track_h2020_sesa = tobac.linking_trackpy(df_h2020_sesa,imerg2020.precip,dt=dt,dxy=dxy,**parameters_linking);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select objects with a lifetime larger than 3 hours\n",
    "## *************** Amazon region  ***************\n",
    "\n",
    "sel_cells_h2015 = sel_by_lifetime(Track_h2015);\n",
    "sel_cells_h2016 = sel_by_lifetime(Track_h2016); \n",
    "sel_cells_h2017 = sel_by_lifetime(Track_h2017);\n",
    "sel_cells_h2018 = sel_by_lifetime(Track_h2018);\n",
    "sel_cells_h2019 = sel_by_lifetime(Track_h2019);\n",
    "sel_cells_h2020 = sel_by_lifetime(Track_h2020);\n",
    "\n",
    "## *************** SESA region  ***************\n",
    "\n",
    "sel_cells_h2015_sesa = sel_by_lifetime(Track_h2015_sesa);\n",
    "sel_cells_h2016_sesa = sel_by_lifetime(Track_h2016_sesa); \n",
    "sel_cells_h2017_sesa = sel_by_lifetime(Track_h2017_sesa);\n",
    "sel_cells_h2018_sesa = sel_by_lifetime(Track_h2018_sesa);\n",
    "sel_cells_h2019_sesa = sel_by_lifetime(Track_h2019_sesa);\n",
    "sel_cells_h2020_sesa = sel_by_lifetime(Track_h2020_sesa);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Redefine masks based on lifetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### load object masks\n",
    "ds_ocs_sesa_2015 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2015_sesa_sizeT2500i1.nc')\n",
    "ds_ocs_sesa_2016 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2016_sesa_sizeT2500i1.nc')\n",
    "ds_ocs_sesa_2017 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2017_sesa_sizeT2500i1.nc')\n",
    "ds_ocs_sesa_2018 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2018_sesa_sizeT2500i1.nc')\n",
    "ds_ocs_sesa_2019 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2019_sesa_sizeT2500i1.nc')\n",
    "ds_ocs_sesa_2020 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2020_sesa_sizeT2500i1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_ocs_2015 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2015_AB_sizeT2500i1.nc')\n",
    "ds_ocs_2016 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2016_AB_sizeT2500i1.nc')\n",
    "ds_ocs_2017 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2017_AB_sizeT2500i1.nc')\n",
    "ds_ocs_2018 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2018_AB_sizeT2500i1.nc')\n",
    "ds_ocs_2019 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2019_AB_sizeT2500i1.nc')\n",
    "ds_ocs_2020 = xr.open_dataset('imerg_masks/ds_ocs_hIMERG2020_AB_sizeT2500i1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_cells_h2015['idxn'] = (pd.to_numeric(sel_cells_h2015['idx'],errors = 'coerce'))\n",
    "sel_cells_h2016['idxn'] = (pd.to_numeric(sel_cells_h2016['idx'],errors = 'coerce'))\n",
    "sel_cells_h2017['idxn'] = (pd.to_numeric(sel_cells_h2017['idx'],errors = 'coerce'))\n",
    "sel_cells_h2018['idxn'] = (pd.to_numeric(sel_cells_h2018['idx'],errors = 'coerce'))\n",
    "sel_cells_h2019['idxn'] = (pd.to_numeric(sel_cells_h2019['idx'],errors = 'coerce'))\n",
    "sel_cells_h2020['idxn'] = (pd.to_numeric(sel_cells_h2020['idx'],errors = 'coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sel_cells_h2015_sesa['idxn'] = (pd.to_numeric(sel_cells_h2015_sesa['idx'],errors = 'coerce'))\n",
    "sel_cells_h2016_sesa['idxn'] = (pd.to_numeric(sel_cells_h2016_sesa['idx'],errors = 'coerce'))\n",
    "sel_cells_h2017_sesa['idxn'] = (pd.to_numeric(sel_cells_h2017_sesa['idx'],errors = 'coerce'))\n",
    "sel_cells_h2018_sesa['idxn'] = (pd.to_numeric(sel_cells_h2018_sesa['idx'],errors = 'coerce'))\n",
    "sel_cells_h2019_sesa['idxn'] = (pd.to_numeric(sel_cells_h2019_sesa['idx'],errors = 'coerce'))\n",
    "sel_cells_h2020_sesa['idxn'] = (pd.to_numeric(sel_cells_h2020_sesa['idx'],errors = 'coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_mask(df,ds_mask):\n",
    "    # Filter the original dataset to only include times that are in the dataframe\n",
    "    original_ds_filtered = ds_mask.sel(time=df['time'].unique())\n",
    "    #new_mask = np.zeros_like(original_ds_filtered.labels_ocs);\n",
    "    \n",
    "    #create boolean\n",
    "    new_mask = [np.isin(original_ds_filtered.sel(time=i)['labels_ocs'],\n",
    "                        df[df.time==i].idxn.values) for i in original_ds_filtered.time.values] \n",
    "    \n",
    "    #make boolean a dataset\n",
    "    ds_new_mask = to_dataset(new_mask,original_ds_filtered)\n",
    "    \n",
    "    #apply to ds_filtered\n",
    "    new_ds = original_ds_filtered.labels_ocs.where(ds_new_mask.mask==True)\n",
    "        \n",
    "\n",
    "    return(new_ds)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nMask_2015_sesa = subset_mask(sel_cells_h2015_sesa,ds_ocs_sesa_2015)\n",
    "nMask_2016_sesa = subset_mask(sel_cells_h2016_sesa,ds_ocs_sesa_2016)\n",
    "nMask_2017_sesa = subset_mask(sel_cells_h2017_sesa,ds_ocs_sesa_2017)\n",
    "nMask_2018_sesa = subset_mask(sel_cells_h2018_sesa,ds_ocs_sesa_2018)\n",
    "nMask_2019_sesa = subset_mask(sel_cells_h2019_sesa,ds_ocs_sesa_2019)\n",
    "nMask_2020_sesa = subset_mask(sel_cells_h2020_sesa,ds_ocs_sesa_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMask_2015 = subset_mask(sel_cells_h2015,ds_ocs_2015)\n",
    "nMask_2016 = subset_mask(sel_cells_h2016,ds_ocs_2016)\n",
    "nMask_2017 = subset_mask(sel_cells_h2017,ds_ocs_2017)\n",
    "nMask_2018 = subset_mask(sel_cells_h2018,ds_ocs_2018)\n",
    "nMask_2019 = subset_mask(sel_cells_h2019,ds_ocs_2019)\n",
    "nMask_2020 = subset_mask(sel_cells_h2020,ds_ocs_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nMask_2015.to_netcdf('imerg_masks/nMask_AB_2015.nc'); \n",
    "# nMask_2016.to_netcdf('imerg_masks/nMask_AB_2016.nc')\n",
    "# nMask_2017.to_netcdf('imerg_masks/nMask_AB_2017.nc'); \n",
    "# nMask_2018.to_netcdf('imerg_masks/nMask_AB_2018.nc')\n",
    "# nMask_2019.to_netcdf('imerg_masks/nMask_AB_2019.nc'); \n",
    "# nMask_2020.to_netcdf('imerg_masks/nMask_AB_2020.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nMask_2015_sesa.to_netcdf('imerg_masks/nMask_sesa_2015.nc'); \n",
    "# nMask_2016_sesa.to_netcdf('imerg_masks/nMask_sesa_2016.nc')\n",
    "# nMask_2017_sesa.to_netcdf('imerg_masks/nMask_sesa_2017.nc'); \n",
    "# nMask_2018_sesa.to_netcdf('imerg_masks/nMask_sesa_2018.nc')\n",
    "# nMask_2019_sesa.to_netcdf('imerg_masks/nMask_sesa_2019.nc'); \n",
    "# nMask_2020_sesa.to_netcdf('imerg_masks/nMask_sesa_2020.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add stage information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stage_convective_system(df_tracked):\n",
    "    \"\"\"\n",
    "    Two main stages are initial t_i and t_f correspondent to first and last timesteps. Intermediate stages\n",
    "    are denoted t_1, t_2,..,t_n\n",
    "    \"\"\"\n",
    "    df = df_tracked[['cell','time_cell']].copy()\n",
    "\n",
    "    # count the number of stages for each cell\n",
    "    n_stages = df.groupby('cell')['time_cell'].count() - 1\n",
    "\n",
    "    # create a dictionary of stages for each cell\n",
    "    stages_dict = {}\n",
    "    for cell in n_stages.index:\n",
    "        \n",
    "        if n_stages.loc[cell] < 7:\n",
    "            stages_dict[cell] = ['t_i'] * 2 + ['t_m'] * (n_stages[cell]-3) + ['t_f'] * 2\n",
    "            \n",
    "        else:            \n",
    "            stages_dict[cell] = ['t_i'] * 3 + ['t_m'] * (n_stages[cell]-5) + ['t_f'] * 3\n",
    "\n",
    "    # create a new column 'stage' based on the cell and the stages dictionary\n",
    "    df['stage'] = [stages_dict[df.loc[i, 'cell']][df.groupby('cell').cumcount()[i]] for i in range(len(df))]\n",
    "    \n",
    "    dfn = df_tracked.reset_index(drop=True);\n",
    "    dfn['stage'] = df['stage']\n",
    "\n",
    "    return(dfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *************** Amazon region  ***************\n",
    "\n",
    "df_stage_h2015 = add_stage_convective_system(sel_cells_h2015)\n",
    "df_stage_h2016 = add_stage_convective_system(sel_cells_h2016);\n",
    "df_stage_h2017 = add_stage_convective_system(sel_cells_h2017);\n",
    "df_stage_h2018 = add_stage_convective_system(sel_cells_h2018);\n",
    "df_stage_h2019 = add_stage_convective_system(sel_cells_h2019);\n",
    "df_stage_h2020 = add_stage_convective_system(sel_cells_h2020);\n",
    "\n",
    "## *************** SESA region  ***************\n",
    "\n",
    "df_stage_h2015_sesa = add_stage_convective_system(sel_cells_h2015_sesa)\n",
    "df_stage_h2016_sesa = add_stage_convective_system(sel_cells_h2016_sesa);\n",
    "df_stage_h2017_sesa = add_stage_convective_system(sel_cells_h2017_sesa);\n",
    "df_stage_h2018_sesa = add_stage_convective_system(sel_cells_h2018_sesa);\n",
    "df_stage_h2019_sesa = add_stage_convective_system(sel_cells_h2019_sesa);\n",
    "df_stage_h2020_sesa = add_stage_convective_system(sel_cells_h2020_sesa);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tools_for_evolution as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add direction propagations\n",
    "\n",
    "## *************** Amazon region  ***************\n",
    "df1_stage_h2015 = df_stage_h2015.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2016 = df_stage_h2016.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2017 = df_stage_h2017.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2018 = df_stage_h2018.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2019 = df_stage_h2019.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2020 = df_stage_h2020.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "\n",
    "df1_stage_h2015['categorized_direction'] = df1_stage_h2015['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2016['categorized_direction'] = df1_stage_h2016['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2017['categorized_direction'] = df1_stage_h2017['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2018['categorized_direction'] = df1_stage_h2018['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2019['categorized_direction'] = df1_stage_h2019['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2020['categorized_direction'] = df1_stage_h2020['direction'].apply(\n",
    "    tools.categorize_direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## *************** SESA region  ***************\n",
    "df1_stage_h2015_sesa = df_stage_h2015_sesa.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2016_sesa = df_stage_h2016_sesa.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2017_sesa = df_stage_h2017_sesa.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2018_sesa = df_stage_h2018_sesa.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2019_sesa = df_stage_h2019_sesa.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "df1_stage_h2020_sesa = df_stage_h2020_sesa.groupby('cell').apply(tools.calculate_direction).reset_index(\n",
    "    drop=True)\n",
    "\n",
    "df1_stage_h2015_sesa['categorized_direction'] = df1_stage_h2015_sesa['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2016_sesa['categorized_direction'] = df1_stage_h2016_sesa['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2017_sesa['categorized_direction'] = df1_stage_h2017_sesa['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2018_sesa['categorized_direction'] = df1_stage_h2018_sesa['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2019_sesa['categorized_direction'] = df1_stage_h2019_sesa['direction'].apply(\n",
    "    tools.categorize_direction)\n",
    "df1_stage_h2020_sesa['categorized_direction'] = df1_stage_h2020_sesa['direction'].apply(\n",
    "    tools.categorize_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add topography details related to OCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tools_for_tobac as toolst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_area = xr.open_dataset('../gridarea_dom03r10.nc')\n",
    "grid_area.coords['lon'] = (grid_area.coords['lon'] + 180) % 360 - 180\n",
    "grid_area = grid_area.sortby(grid_area.lon)\n",
    "grid_area = grid_area.interp(lat = imerg2015.lat.values,lon = imerg2015.lon.values)\n",
    "# grid_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path='/scratch/wcq7pz/exp_levante_post/'\n",
    "## open topography, land_fraction\n",
    "topo5km = xr.open_dataset(path+'topography_dom03_5km.nc')\n",
    "topo5km.coords['lon'] = (topo5km.coords['lon'] + 180) % 360 - 180\n",
    "topo5km  = topo5km.sortby(topo5km.lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topo10km = topo5km.interp(lat = imerg2020.lat.values,lon = imerg2020.lon.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Repeat the variable along the time dimension of ds2\n",
    "ds_topo_2015 = topo10km['topography_c'].broadcast_like(imerg2015['time'])\n",
    "ds_topo_2016 = topo10km['topography_c'].broadcast_like(imerg2016['time'])\n",
    "ds_topo_2017 = topo10km['topography_c'].broadcast_like(imerg2017['time'])\n",
    "ds_topo_2018 = topo10km['topography_c'].broadcast_like(imerg2018['time'])\n",
    "ds_topo_2019 = topo10km['topography_c'].broadcast_like(imerg2019['time'])\n",
    "ds_topo_2020 = topo10km['topography_c'].broadcast_like(imerg2020['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the 'mask' variable to only include the values present in the original dataframe\n",
    "# del(ab_topo2015,ab_topo2016,ab_topo2017,ab_topo2018,ab_topo2019,ab_topo2020)\n",
    "ab_topo2015 =  ds_topo_2015.where(ds_topo_2015['time'].isin(nMask_2015.time), drop=True)\n",
    "ab_topo2016 =  ds_topo_2016.where(ds_topo_2016['time'].isin(nMask_2016.time), drop=True)\n",
    "ab_topo2017 =  ds_topo_2017.where(ds_topo_2017['time'].isin(nMask_2017.time), drop=True)\n",
    "ab_topo2018 =  ds_topo_2018.where(ds_topo_2018['time'].isin(nMask_2018.time), drop=True)\n",
    "ab_topo2019 =  ds_topo_2019.where(ds_topo_2019['time'].isin(nMask_2019.time), drop=True)\n",
    "ab_topo2020 =  ds_topo_2020.where(ds_topo_2020['time'].isin(nMask_2020.time), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## *************** SESA region  ***************\n",
    "sesa_topo_2015 =  ds_topo_2015.where(ds_topo_2015['time'].isin(nMask_2015_sesa.time), drop=True)\n",
    "sesa_topo_2016 =  ds_topo_2016.where(ds_topo_2016['time'].isin(nMask_2016_sesa.time), drop=True)\n",
    "sesa_topo_2017 =  ds_topo_2017.where(ds_topo_2017['time'].isin(nMask_2017_sesa.time), drop=True)\n",
    "sesa_topo_2018 =  ds_topo_2018.where(ds_topo_2018['time'].isin(nMask_2018_sesa.time), drop=True)\n",
    "sesa_topo_2019 =  ds_topo_2019.where(ds_topo_2019['time'].isin(nMask_2019_sesa.time), drop=True)\n",
    "sesa_topo_2020 =  ds_topo_2020.where(ds_topo_2020['time'].isin(nMask_2020_sesa.time), drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### extract arrays\n",
    "##del(arraySFC_control)#,arraySFC_fixedSM) Removed .where((maskAB.Band1>0),-1) after slicing\n",
    "arrayTOPO_ab_2015 = np.squeeze(ab_topo2015.sel(\n",
    "    lon=slice(nMask_2015.lon.min(),nMask_2015.lon.max()),lat=slice(nMask_2015.lat.min(),nMask_2015.lat.max())).values)\n",
    "arrayTOPO_ab_2016 = np.squeeze(ab_topo2016.sel(\n",
    "    lon=slice(nMask_2016.lon.min(),nMask_2016.lon.max()),lat=slice(nMask_2016.lat.min(),nMask_2016.lat.max())).values)\n",
    "arrayTOPO_ab_2017 = np.squeeze(ab_topo2017.sel(\n",
    "    lon=slice(nMask_2017.lon.min(),nMask_2017.lon.max()),lat=slice(nMask_2017.lat.min(),nMask_2017.lat.max())).values)\n",
    "arrayTOPO_ab_2018 = np.squeeze(ab_topo2018.sel(\n",
    "    lon=slice(nMask_2018.lon.min(),nMask_2018.lon.max()),lat=slice(nMask_2018.lat.min(),nMask_2018.lat.max())).values)\n",
    "arrayTOPO_ab_2019 = np.squeeze(ab_topo2019.sel(\n",
    "    lon=slice(nMask_2019.lon.min(),nMask_2019.lon.max()),lat=slice(nMask_2019.lat.min(),nMask_2019.lat.max())).values)\n",
    "arrayTOPO_ab_2020 = np.squeeze(ab_topo2020.sel(\n",
    "    lon=slice(nMask_2020.lon.min(),nMask_2020.lon.max()),lat=slice(nMask_2020.lat.min(),nMask_2020.lat.max())).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######### ********* SESA REGION\n",
    "\n",
    "def make_topo_array(topo_arr,mask_arr):\n",
    "    array_sesa = (topo_arr.sel(lon=slice(mask_arr.lon.min(),mask_arr.lon.max()),lat=slice(mask_arr.lat.min(),mask_arr.lat.max())))\n",
    "    # Check if all grid points for each time step are equal to 0\n",
    "    all_zeros = (array_sesa == 0).all(dim=('lat', 'lon'))\n",
    "    # Replace the grid points with 1 where all values are 0\n",
    "    array_sesa = array_sesa.where(~all_zeros, 1)\n",
    "    return(array_sesa.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arrayTOPO_sesa_2015 = make_topo_array(sesa_topo_2015,nMask_2015_sesa)\n",
    "arrayTOPO_sesa_2016 = make_topo_array(sesa_topo_2016,nMask_2016_sesa)\n",
    "arrayTOPO_sesa_2017 = make_topo_array(sesa_topo_2017,nMask_2017_sesa)\n",
    "arrayTOPO_sesa_2018 = make_topo_array(sesa_topo_2018,nMask_2018_sesa)\n",
    "arrayTOPO_sesa_2019 = make_topo_array(sesa_topo_2019,nMask_2019_sesa)\n",
    "arrayTOPO_sesa_2020 = make_topo_array(sesa_topo_2020,nMask_2020_sesa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sel_gridarea(gridarea,mask):\n",
    "    return(gridarea.sel(lon=slice(mask.lon.min(),mask.lon.max()), \n",
    "                        lat=slice(mask.lat.min(),mask.lat.max())).cell_area.values / 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_sesa_2015 = toolst.mask_var(nMask_2015_sesa,arrayTOPO_sesa_2015,\n",
    "      sel_gridarea(grid_area,nMask_2015_sesa),lon1=nMask_2015_sesa.coords['lon'].values,\n",
    "                        lat1=nMask_2015_sesa.coords['lat'].values,rr_limit=0,\n",
    "                        timeds=pd.Series(nMask_2015_sesa.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_sesa_2016 = toolst.mask_var(nMask_2016_sesa,arrayTOPO_sesa_2016,sel_gridarea(grid_area,nMask_2016_sesa),\n",
    "                        lon1=nMask_2016_sesa.coords['lon'].values,\n",
    "                        lat1=nMask_2016_sesa.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2016_sesa.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_sesa_2017 = toolst.mask_var(nMask_2017_sesa,arrayTOPO_sesa_2017,sel_gridarea(grid_area,nMask_2017_sesa),\n",
    "                        lon1=nMask_2017_sesa.coords['lon'].values,\n",
    "                        lat1=nMask_2017_sesa.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2017_sesa.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_sesa_2018 = toolst.mask_var(nMask_2018_sesa,arrayTOPO_sesa_2018,sel_gridarea(grid_area,nMask_2018_sesa),\n",
    "                        lon1=nMask_2018_sesa.coords['lon'].values,\n",
    "                        lat1=nMask_2018_sesa.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2018_sesa.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_sesa_2019 = toolst.mask_var(nMask_2019_sesa,arrayTOPO_sesa_2019,sel_gridarea(grid_area,nMask_2019_sesa),\n",
    "                        lon1=nMask_2019_sesa.coords['lon'].values,\n",
    "                        lat1=nMask_2019_sesa.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2019_sesa.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_sesa_2020 = toolst.mask_var(nMask_2020_sesa,arrayTOPO_sesa_2020,sel_gridarea(grid_area,nMask_2020_sesa),\n",
    "                        lon1=nMask_2020_sesa.coords['lon'].values,\n",
    "                        lat1=nMask_2020_sesa.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2020_sesa.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_sesa_2015.to_pickle('imerg_pkl/df_topo_sesa_2015.pkl')\n",
    "df_topo_sesa_2016.to_pickle('imerg_pkl/df_topo_sesa_2016.pkl')\n",
    "df_topo_sesa_2017.to_pickle('imerg_pkl/df_topo_sesa_2017.pkl')\n",
    "df_topo_sesa_2018.to_pickle('imerg_pkl/df_topo_sesa_2018.pkl')\n",
    "df_topo_sesa_2019.to_pickle('imerg_pkl/df_topo_sesa_2019.pkl')\n",
    "df_topo_sesa_2020.to_pickle('imerg_pkl/df_topo_sesa_2020.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### AMAZON region\n",
    "df_topo_ab_2015 = toolst.mask_var(nMask_2015,arrayTOPO_ab_2015,sel_gridarea(grid_area,nMask_2015),\n",
    "                      lon1=nMask_2015.coords['lon'].values,\n",
    "                      lat1=nMask_2015.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2015.time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_ab_2016 = toolst.mask_var(nMask_2016,arrayTOPO_ab_2016,sel_gridarea(grid_area,nMask_2016),\n",
    "                      lon1=nMask_2016.coords['lon'].values,\n",
    "                      lat1=nMask_2016.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2016.time))\n",
    "df_topo_ab_2017 = toolst.mask_var(nMask_2017,arrayTOPO_ab_2017,sel_gridarea(grid_area,nMask_2017),\n",
    "                      lon1=nMask_2017.coords['lon'].values,\n",
    "                      lat1=nMask_2017.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2017.time))\n",
    "df_topo_ab_2018 = toolst.mask_var(nMask_2018,arrayTOPO_ab_2018,sel_gridarea(grid_area,nMask_2018),\n",
    "                      lon1=nMask_2018.coords['lon'].values,\n",
    "                      lat1=nMask_2018.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2018.time))\n",
    "df_topo_ab_2019 = toolst.mask_var(nMask_2019,arrayTOPO_ab_2019,sel_gridarea(grid_area,nMask_2019),\n",
    "                      lon1=nMask_2019.coords['lon'].values,\n",
    "                      lat1=nMask_2019.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2019.time))\n",
    "df_topo_ab_2020 = toolst.mask_var(nMask_2020,arrayTOPO_ab_2020,sel_gridarea(grid_area,nMask_2020),\n",
    "                      lon1=nMask_2020.coords['lon'].values,\n",
    "                      lat1=nMask_2020.coords['lat'].values,rr_limit=0,timeds=pd.Series(nMask_2020.time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_ab_2015.to_pickle('imerg_pkl/df_topo_ab_2015.pkl')\n",
    "df_topo_ab_2016.to_pickle('imerg_pkl/df_topo_ab_2016.pkl')\n",
    "df_topo_ab_2017.to_pickle('imerg_pkl/df_topo_ab_2017.pkl')\n",
    "df_topo_ab_2018.to_pickle('imerg_pkl/df_topo_ab_2018.pkl')\n",
    "df_topo_ab_2019.to_pickle('imerg_pkl/df_topo_ab_2019.pkl')\n",
    "df_topo_ab_2020.to_pickle('imerg_pkl/df_topo_ab_2020.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dataframes of OCS and topo information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_topo_ab_2015 = pd.read_pickle('imerg_pkl/df_topo_ab_2015.pkl')\n",
    "df_topo_ab_2016 = pd.read_pickle('imerg_pkl/df_topo_ab_2016.pkl')\n",
    "df_topo_ab_2017 = pd.read_pickle('imerg_pkl/df_topo_ab_2017.pkl')\n",
    "df_topo_ab_2018 = pd.read_pickle('imerg_pkl/df_topo_ab_2018.pkl')\n",
    "df_topo_ab_2019 = pd.read_pickle('imerg_pkl/df_topo_ab_2019.pkl')\n",
    "df_topo_ab_2020 = pd.read_pickle('imerg_pkl/df_topo_ab_2020.pkl')\n",
    "\n",
    "df_topo_sesa_2015 = pd.read_pickle('imerg_pkl/df_topo_sesa_2015.pkl')\n",
    "df_topo_sesa_2016 = pd.read_pickle('imerg_pkl/df_topo_sesa_2016.pkl')\n",
    "df_topo_sesa_2017 = pd.read_pickle('imerg_pkl/df_topo_sesa_2017.pkl')\n",
    "df_topo_sesa_2018 = pd.read_pickle('imerg_pkl/df_topo_sesa_2018.pkl')\n",
    "df_topo_sesa_2019 = pd.read_pickle('imerg_pkl/df_topo_sesa_2019.pkl')\n",
    "df_topo_sesa_2020 = pd.read_pickle('imerg_pkl/df_topo_sesa_2020.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_cols(df,dfsm,name_mean='topo_mean',name_median='topo_median',name_std='topo_std',name_max='topo_max'):\n",
    "    df[name_mean] = pd.to_numeric(dfsm['mean'],errors = 'coerce')\n",
    "    df[name_median] = pd.to_numeric(dfsm['median'],errors = 'coerce')\n",
    "    df[name_max] = pd.to_numeric(dfsm['max'],errors = 'coerce')\n",
    "    df[name_std] = pd.to_numeric(dfsm['std'],errors = 'coerce')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801 1801\n"
     ]
    }
   ],
   "source": [
    "print(len(df1_stage_h2015_sesa),len(df_topo_sesa_2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1_stage_h2015_sesa = add_cols(df1_stage_h2015_sesa,df_topo_sesa_2015)\n",
    "df1_stage_h2016_sesa = add_cols(df1_stage_h2016_sesa,df_topo_sesa_2016)\n",
    "df1_stage_h2017_sesa = add_cols(df1_stage_h2017_sesa,df_topo_sesa_2017)\n",
    "df1_stage_h2018_sesa = add_cols(df1_stage_h2018_sesa,df_topo_sesa_2018)\n",
    "df1_stage_h2019_sesa = add_cols(df1_stage_h2019_sesa,df_topo_sesa_2019)\n",
    "df1_stage_h2020_sesa = add_cols(df1_stage_h2020_sesa,df_topo_sesa_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1_stage_h2015_ab = add_cols(df1_stage_h2015,df_topo_ab_2015)\n",
    "df1_stage_h2016_ab = add_cols(df1_stage_h2016,df_topo_ab_2016)\n",
    "df1_stage_h2017_ab = add_cols(df1_stage_h2017,df_topo_ab_2017)\n",
    "df1_stage_h2018_ab = add_cols(df1_stage_h2018,df_topo_ab_2018)\n",
    "df1_stage_h2019_ab = add_cols(df1_stage_h2019,df_topo_ab_2019)\n",
    "df1_stage_h2020_ab = add_cols(df1_stage_h2020,df_topo_ab_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1_stage_h2015_ab.to_pickle('imerg_pkl/df_imerg_stage_AB_2015.pkl')\n",
    "df1_stage_h2016_ab.to_pickle('imerg_pkl/df_imerg_stage_AB_2016.pkl')\n",
    "df1_stage_h2017_ab.to_pickle('imerg_pkl/df_imerg_stage_AB_2017.pkl')\n",
    "df1_stage_h2018_ab.to_pickle('imerg_pkl/df_imerg_stage_AB_2018.pkl')\n",
    "df1_stage_h2019_ab.to_pickle('imerg_pkl/df_imerg_stage_AB_2019.pkl')\n",
    "df1_stage_h2020_ab.to_pickle('imerg_pkl/df_imerg_stage_AB_2020.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1_stage_h2015_sesa.to_pickle('imerg_pkl/df_imerg_stage_SESA_2015.pkl')\n",
    "df1_stage_h2016_sesa.to_pickle('imerg_pkl/df_imerg_stage_SESA_2016.pkl')\n",
    "df1_stage_h2017_sesa.to_pickle('imerg_pkl/df_imerg_stage_SESA_2017.pkl')\n",
    "df1_stage_h2018_sesa.to_pickle('imerg_pkl/df_imerg_stage_SESA_2018.pkl')\n",
    "df1_stage_h2019_sesa.to_pickle('imerg_pkl/df_imerg_stage_SESA_2019.pkl')\n",
    "df1_stage_h2020_sesa.to_pickle('imerg_pkl/df_imerg_stage_SESA_2020.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_LP2",
   "language": "python",
   "name": "cloned_myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
